# tinyGPT
A tiny GPT model inspired by karpathy's nanoGPT and OpenAI's gpt-2.  
This repo deals with pre-training.

TODOs:
check correctness of arch, rot emb, kv cache > hyperparam figure out > inference code > mfu calculation > then good data and final training run > good proj structure > awesome readme and blog > better publish > cuda kernel improvements

Done: 
batch size > Optim formula > then code/debug > param counts/and training time on gpus > 

Run this command to start the training
```
bash run.sh
```
