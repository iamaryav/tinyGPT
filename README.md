# tinyGPT

A tiny GPT model inspired by karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT) and OpenAI's GPT-2. This repo deals with pre-training. Currently in this repo 4 different types of architectures are present. Checkout my blog [Build GPT from scratch](), where I explained in detail about concepts used in GPT architecture and training.  

1. bigram
2. gpt
3. gpt2
4. qwen2.5

To run qwen2 training run this command
```
bash ./runs/run.sh
```
`run.sh` file has all the steps to start pretraining the model. If you want to run manually go through the `./runs/run.sh` to see what are the things needed to manually run the model. To run the training with different datasets and param config comment/uncomment in `run.sh` file.  

Once training is complete run below command to interact with the model.  
```
python -m tinygpt.out
```

Sample generated by model after training e.g.  
```
CAMILLO:
There is a sickness
Which puts some of us in distemper, but
I cannot name the like.

POLIXENES:
No longer.

POLIXENES:
A sickness caught of me, and yet you;
I mean better.


POLIXENES:
A caught of me, and haveAs heavens caught
So young met thus; no, to mean better:
Which 'twere my daughter,
You never spoke what did become you'll procure that honesty
 life born.
```

To run the GPT-2 training
```
python -m scripts.train_gpt2
```

To run the plain GPT training run this command
```
python -m tinygpt.gpt
```

To run the bigram model run below command
```
python -m tinygpt.bigram
```

