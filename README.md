# tinyGPT
A tiny GPT model inspired by karpathy's nanoGPT and OpenAI's gpt-2.  
This repo deals with pre-training.

TODOs:
hyperparam, refactoring and final training run > updated readme > blog/posts/publish 

Done: 
batch size > Optim formula > then code/debug > param counts/and training time on gpus > check correctness of arch, rot emb, kv cache > inference code > mfu calculation 
Run this command to start the training


```
bash run.sh
```