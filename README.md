# tinyGPT
A tiny GPT model inspired by karpathy's nanoGPT and OpenAI's gpt-2.  
This repo deals with pre-training.

TODOs:
hyperparam figure out, then good data and final training run > good proj structure > awesome readme and blog > better publish > cuda kernel improvements

Done: 
batch size > Optim formula > then code/debug > param counts/and training time on gpus > check correctness of arch, rot emb, kv cache > inference code > mfu calculation 
Run this command to start the training


```
bash run.sh
```