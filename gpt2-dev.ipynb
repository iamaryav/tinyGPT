{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete tinyGPT\n",
    "# Then build and train gpt2\n",
    "# Papers - Attention is all you need\n",
    "# GPT 2/3 paper\n",
    "\n",
    "# Llamac/llama2.c\n",
    "# Cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to?\n",
    "# Model arcthitecture -> \n",
    "# Data(find and filter) -> \n",
    "# train -> \n",
    "# testing/benchmarks -> \n",
    "# Sample\n",
    "# Some other evaluations and researches like scaling laws, transformer sizing\n",
    "# Configurator\n",
    "# Play with some opensource model also\n",
    "# pretraining | post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train model from as small as one to context length\n",
    "# so model will see all kinds of context length and from 1 to length of context(block size)\n",
    "# when predicting the character\n",
    "# batch - for parallel processing\n",
    "# In multidimensional matrix\n",
    "# Each dimension is axis and each axis has index\n",
    "#dim=0 → collapsing rows → gives a summary per column\n",
    "# dim=1 → collapsing columns → gives a summary per row\n",
    "# [a, b, c] = [a][b][c] : same in slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBigramNeuralNetwork\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# define how the neural network looks like\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# their structure\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# input layer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# middle layer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# output layer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "# Lets create a BigramNeuralNetwork class with attention in it\n",
    "vocab_size = 8\n",
    "n_embd = 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class BigramNeuralNetwork(torch.nn):\n",
    "    def __init__(self):\n",
    "        # define how the neural network looks like\n",
    "        # their structure\n",
    "        # input layer\n",
    "        # middle layer\n",
    "        # output layer\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        logits = self.lm_head(tok_emb) # (B, T, vocab_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Attention is all you need paper\n",
    "class Block:\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "torch.Size([4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# B - Batch - parallel input sequence\n",
    "# T - Time(vocab size, sequence length, memory used in prediction, Context size) - position of tokens\n",
    "# c - channel(embedding dimensions, d_model) - Contains feature vector for each token\n",
    "# torch.tril(torch.ones(T, T))\n",
    "# embedding table contains feature vector for each token\n",
    "# each row corresponds to a token and columns feature vector\n",
    "# embedding table dimensions - (vocab_size, Channel)\n",
    "# Each token becomes three vectors: \n",
    "# Query: what this token is looking for?\n",
    "# Key: what this token has to offer\n",
    "# Value: what the information this token contains for sharing to next token i thinks\n",
    "# Encoder model: Takes all the tokens in consideration\n",
    "# Decoder model: Takes only current and past tokens in consideration\n",
    "# Query @ Key = Scores\n",
    "# Scale - Stabilize scores\n",
    "# Softmax(scores) = weights \n",
    "# Weighted average of values - New token representations\n",
    "# Scores: strong positive value - strong match high attention\n",
    "# Near zero: No match - Low attention\n",
    "# Negative number: Disimilar - very low ignored after softmax\n",
    "# Linear Layer is simple matrix multiplication- input @ weight + bias = w @ x + b\n",
    "# Training decides that what query, key and value will contain\n",
    "# Mainly backpropagation and gradient descent this is amazing\n",
    "# embedding dimensions or channel dimensions or feature vector of a token\n",
    "# is broken down in to key, query and vector and we train these three things\n",
    "# and after training these things contains meaning representation of embedding dimensions\n",
    "# query of current token, and key and values of all the past token\n",
    "# Keep the dimensions of k, q, v so dot products should be easy\n",
    "# Mostly same or half\n",
    "# in multi head attention keep the dimension as embd_d // head_size\n",
    "#  The dot product\n",
    "# the dot prodcut in between |A||B|cos(theta)\n",
    "# In Other words: Projection of a on b\n",
    "# IOW: how much A aligns with the B\n",
    "# iow: how much a is in the direction of B\n",
    "# iow: similarity between two vectors\n",
    "\n",
    "import torch\n",
    "B, T, C = 4, 8, 32 # consider this as stacked sheet of paper\n",
    "x = torch.randn(B, T, C)\n",
    "key = nn.Linear(32, 16)\n",
    "print(key.weight.shape)\n",
    "query = nn.Linear(32, 16)\n",
    "k = key(x) # 4, 8, 16\n",
    "q = query(x) # 4, 8, 16\n",
    "out = (k @ q.transpose(-2, -1)) / k[-1].shape ** 0.5 # (4, 8, 16) @ (4, 16, 8) -> 4, 8, 8\n",
    "print(out.shape)\n",
    "# print(key(x).shape) # 4, 8, 32 @ 4, 32, 16 ---> 4, 8, 16 \n",
    "# ex = nn.Linear(3, 5)\n",
    "# print(x2.weight)\n",
    "# x11 = x1(x)\n",
    "# y11 = x2(x)\n",
    "# y11.shape\n",
    "# x11 @ y11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "tensor([[-0.5116,  0.1790,  0.1477,  0.4020, -0.5156,  0.0225, -0.2813,  0.0107,\n",
      "          0.6123, -0.4112, -0.2374,  0.3414, -0.2318,  0.0701,  0.1628, -1.2048],\n",
      "        [-0.4916,  0.1789,  0.1393,  0.3719, -0.5013,  0.0254, -0.2581,  0.0073,\n",
      "          0.5818, -0.3724, -0.2384,  0.2642, -0.2258,  0.0303,  0.1463, -1.0543],\n",
      "        [-0.2618,  0.1780,  0.0428,  0.0266, -0.3375,  0.0586,  0.0081, -0.0322,\n",
      "          0.2317,  0.0728, -0.2498, -0.6218, -0.1567, -0.4260, -0.0434,  0.6724],\n",
      "        [-0.4935,  0.1789,  0.1402,  0.3750, -0.5027,  0.0251, -0.2604,  0.0076,\n",
      "          0.5849, -0.3763, -0.2383,  0.2720, -0.2264,  0.0343,  0.1480, -1.0696],\n",
      "        [-0.2504,  0.1766,  0.2284,  0.3013, -0.4933, -0.0036, -0.1632, -0.1655,\n",
      "          0.5580, -0.2457, -0.2732,  0.3370, -0.1941,  0.0295,  0.1574, -1.0176],\n",
      "        [ 1.1250,  0.2218,  1.1225, -0.0294, -0.3124,  0.3271, -0.0952, -1.3758,\n",
      "          0.1166,  0.2256, -0.4738,  0.3247,  0.1788, -0.0392,  0.0256, -0.4580],\n",
      "        [-0.9924,  0.9556, -0.3172, -0.2451,  0.8941,  0.2751, -0.5098,  0.1941,\n",
      "         -0.8036, -0.8719,  1.0875, -1.2967,  0.1949,  0.1715, -0.0909,  0.2874],\n",
      "        [ 1.2540,  0.3052,  1.3663, -0.0572, -0.0414,  0.6327, -0.3395, -1.6170,\n",
      "         -0.2690,  0.0543, -0.3431, -0.0517,  0.4211, -0.0400, -0.1169, -0.0635]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "B, T, C = 4, 8, 32\n",
    "vocab_size = 64 # How many tokens?\n",
    "batch_size = 4 # input sequence we will processed in parallel\n",
    "block_size = 8 # Context length\n",
    "embd_d = 32 # Feature vector to represent a token\n",
    "head_size = 16 # size of head\n",
    "x = torch.randn(B, T, C) # Input data\n",
    "\n",
    "# One Head Attention Block\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embd_d, head_size, bias=False) # y = x @ wT + b\n",
    "        self.query = nn.Linear(embd_d, head_size, bias=False)\n",
    "        self.value = nn.Linear(embd_d, head_size, bias=False)\n",
    "        # to make lower triangle that will help in weighted average calculation\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** 0.5 # (B, T, head_size) @ (B, head_size) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T), deactivating next tokens\n",
    "        wei = F.softmax(wei, dim= -1) # (B, T, T)\n",
    "        # Weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "attention = Head(head_size)\n",
    "y = attention(x)\n",
    "print(y.shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
